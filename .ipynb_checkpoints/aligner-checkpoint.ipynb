{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Linda\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "##will soon become a python file\n",
    "\n",
    "import numpy as np\n",
    "import gensim \n",
    "import nltk\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import logging\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import dot, zeros, dtype, float32 as REAL,\\\n",
    "    double, array, vstack, fromstring, sqrt, newaxis,\\\n",
    "    ndarray, sum as np_sum, prod, ascontiguousarray,\\\n",
    "    argmax\n",
    "from pyemd import emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(sentence,language):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = []\n",
    "    words = sentence.split(' ')\n",
    "    for w in words:\n",
    "        #remove manually words with single quote\n",
    "        if w == \"you're\" or w == \"i'll\" or w == \"we're\" or w == \"i'm\" or w == \"he's\" or w == \"she's\" or w == \"they're\" :\n",
    "            pass\n",
    "        elif w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load text and treat\n",
    "def load_care(path): \n",
    "    text_en = []\n",
    "    aux = open(path,encoding = 'utf8').readlines()\n",
    "    for item in aux:\n",
    "        item = item.lower()\n",
    "        stringlist = []\n",
    "        item = item.replace(\"'\",'')\n",
    "        for letter in item:  \n",
    "            if letter == ',' or letter == '.' or letter == ';' or letter == '(' or letter == ')' or letter == '/' or letter == '\"\"\"' or letter == '\"\\\"' or letter == '\"' or letter == '?' or letter == '!':\n",
    "                letter = ''\n",
    "            elif letter.isdigit() == True:\n",
    "                letter = ''\n",
    "            stringlist.append(letter)\n",
    "        text_en.append(''.join(stringlist))\n",
    "        \n",
    "    return text_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '~/desktop/embeddings/wiki.en.align.vec'\n",
    "tgt_path = '~/desktop/embeddings/wiki.de.align.vec'\n",
    "nmax = 150000  \n",
    "src_model = gensim.models.KeyedVectors.load_word2vec_format(src_path,limit = nmax)\n",
    "tgt_model = gensim.models.KeyedVectors.load_word2vec_format(tgt_path,limit = nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedder(sentence,model,language):                                                                     \n",
    "    first_word = 0 \n",
    "    word_emb = []\n",
    "    sentence_test = stop_words(sentence,language)\n",
    "    if len(sentence_test)>1:                              #do not apply stopwords for really short sentences\n",
    "        sentence = sentence_test\n",
    "    \n",
    "    for word in sentence[:len(sentence)-2]:               #delete \\n -> last 2 chars                                \n",
    "        if first_word == 0: \n",
    "            if word in model:\n",
    "                word_emb = model[word]\n",
    "                first_word = 1\n",
    "        else:\n",
    "            if word in model:\n",
    "                word_emb = word_emb + model[word]   \n",
    "    \n",
    "    return  word_emb    #np.append(word_emb,len(sentence))  returns the word embedding plus its size(300 + 1)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedder(sentence_list,model,language):\n",
    "    emb = []\n",
    "    for sentence in sentence_list:\n",
    "        save_last = sentence\n",
    "        if(len(sentence)>1):\n",
    "            emb.append(embedder(sentence,model,language))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = load_care('./sentences/src-train.txt')\n",
    "tgt_list = load_care('./sentences/tgt-train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences = sentence_embedder(src_list[:100],src_model,'english')\n",
    "tgt_sentences = sentence_embedder(tgt_list[:100],tgt_model,'german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrix(src_sentences,tgt_sentences):\n",
    "    matriz = np.zeros((len(src_sentences),len(tgt_sentences)))\n",
    "    for i in range(0,len(src_sentences)):\n",
    "        listou = []\n",
    "        for j in range(0,len(tgt_sentences)):\n",
    "             listou.append(1-spatial.distance.cosine(src_sentences[i],tgt_sentences[j]))\n",
    "        matriz = np.insert(matriz,i,listou,0)\n",
    "        np.savetxt(\"result_matrix.csv\",matriz, delimiter=\",\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_matrix(src_sentences,tgt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmdistance_b(src_model,tgt_model, document1, document2):\n",
    "        \"\"\"\n",
    "        .. Ofir Pele and Michael Werman, \"A linear time histogram metric for improved SIFT matching\".\n",
    "        .. Ofir Pele and Michael Werman, \"Fast and robust earth mover's distances\".\n",
    "        .. Matt Kusner et al. \"From Word Embeddings To Document Distances\".\n",
    "        Note that if one of the documents have no words that exist in the\n",
    "        Word2Vec vocab, `float('inf')` (i.e. infinity) will be returned.\n",
    "        This method only works if `pyemd` is installed (can be installed via pip, but requires a C compiler).\n",
    "       \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        PYEMD_EXT = True\n",
    "        if not PYEMD_EXT:\n",
    "            raise ImportError(\"Please install pyemd Python package to compute WMD.\")\n",
    "\n",
    "        # Remove out-of-vocabulary words.\n",
    "        len_pre_oov1 = len(document1)\n",
    "        len_pre_oov2 = len(document2)\n",
    "        document1 = [token for token in document1 if token in src_model]\n",
    "        document2 = [token for token in document2 if token in tgt_model]\n",
    "        diff1 = len_pre_oov1 - len(document1)\n",
    "        diff2 = len_pre_oov2 - len(document2)\n",
    "        if diff1 > 0 or diff2 > 0:\n",
    "            logger.info('Removed %d and %d OOV words from document 1 and 2 (respectively).', diff1, diff2)\n",
    "\n",
    "        if len(document1) == 0 or len(document2) == 0:\n",
    "            logger.info(\n",
    "                \"At least one of the documents had no words that werein the vocabulary. \"\n",
    "                \"Aborting (returning inf).\"\n",
    "            )\n",
    "            return float('inf')\n",
    "\n",
    "        dictionary = Dictionary(documents=[document1, document2])\n",
    "        vocab_len = len(dictionary)\n",
    "\n",
    "        if vocab_len == 1:\n",
    "            # Both documents are composed by a single unique token\n",
    "            return 0.0\n",
    "\n",
    "        # Sets for faster look-up.\n",
    "        docset1 = set(document1)\n",
    "        docset2 = set(document2)\n",
    "        # Compute distance matrix.\n",
    "        distance_matrix = zeros((vocab_len, vocab_len), dtype=double)\n",
    "        for i, t1 in dictionary.items():\n",
    "            for j, t2 in dictionary.items():\n",
    "                if t1 not in docset1 or t2 not in docset2:\n",
    "                    continue\n",
    "                # Compute Euclidean distance between word vectors.\n",
    "                distance_matrix[i, j] = sqrt(np_sum((src_model[t1] - tgt_model[t2])**2))\n",
    "\n",
    "        if np_sum(distance_matrix) == 0.0:\n",
    "            # `emd` gets stuck if the distance matrix contains only zeros.\n",
    "            logger.info('The distance matrix is all zeros. Aborting (returning inf).')\n",
    "            return float('inf')\n",
    "       \n",
    "        def nbow(document):\n",
    "            d = zeros(vocab_len, dtype=double)\n",
    "            nbow = dictionary.doc2bow(document)  # Word frequencies.\n",
    "            doc_len = len(document)\n",
    "            for idx, freq in nbow:\n",
    "                d[idx] = freq / float(doc_len)  # Normalized word frequencies.\n",
    "            return d\n",
    "\n",
    "        # Compute nBOW representation of documents.\n",
    "        d1 = nbow(document1)\n",
    "        d2 = nbow(document2)\n",
    "\n",
    "        # Compute WMD.\n",
    "        return emd(d1, d2, distance_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
