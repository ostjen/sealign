{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading pre-trained common-space embeddings(load_vec function found at MUSE demo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '/Users/linda/desktop/embeddings/wiki.en.align.vec'\n",
    "tgt_path = '/Users/linda/desktop/embeddings/wiki.es.align.vec'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opening text file and transforming tokens into a list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = open(\"/Users/linda/desktop/embedding_align/teste.txt\",\"r\",encoding=\"utf8\").read() #tokenized text\n",
    "text_en = text_en.split('ï¿­')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming Sentences To Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {v: k for k, v in src_id2word.items()}\n",
    "em_list = []\n",
    "\n",
    "for sentences in text_en:                                                                      \n",
    "    first_word = 0\n",
    "    sentences = sentences.split(' ')\n",
    "    for word in sentences:\n",
    "        if(word2id.get(word) != None):                                 \n",
    "            if (first_word == 0):\n",
    "                word_emb = src_embeddings[word2id[word]]\n",
    "                first_word = 1\n",
    "            else:\n",
    "                word_emb += src_embeddings[word2id[word]]    \n",
    "                \n",
    "        \n",
    "    em_list.append(word_emb)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
